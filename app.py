"""
SEO Article Outline Generator
-----------------------------
* VyhledÃ¡ TOP 3 vÃ½sledky Googlu pÅ™es SerpAPI.
* StÃ¡hne jejich HTML a analyzuje klÃ­ÄovÃ¡ slova (frekvence + TF-IDF LSI).
* Vygeneruje pouze **osnovu** ÄlÃ¡nku (H-headery, bullet-pointy),
  meta-title, meta-description a nÃ¡vrhy internÃ­ch odkazÅ¯.

PotÅ™ebnÃ© promÄ›nnÃ© prostÅ™edÃ­ / Streamlit Secrets:
  SERPAPI_API_KEY
  OPENAI_API_KEY
"""

import os
import re
import requests
from collections import Counter, defaultdict

import streamlit as st
from bs4 import BeautifulSoup
from serpapi import GoogleSearch            # sprÃ¡vnÃ½ namespace
from openai import OpenAI
import tldextract

# NLP & ML
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.stem.snowball import SnowballStemmer  # univerzÃ¡lnÃ­ stemmer

nltk.download("punkt", quiet=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API klÃ­Äe
SERP_API_KEY = os.getenv("SERPAPI_API_KEY")
client = OpenAI()                            # naÄte OPENAI_API_KEY z env

# Stop-slova (CZ + EN, zÃ¡klad)
STOP_WORDS = set("""
the a an and or of to in on for with is are be as at by this that from it its
will was were has have had but not your you
a i k o u s v z na Å¾e se je jsou by byl byla bylo aby do od po pro pod nad
kterÃ½ kterÃ¡ kterÃ© co to toto tyto ten ta tÃ­m tuto tu jako kde kdy jak tak takÃ© bez
""".split())

CS_STEM = SnowballStemmer("czech")
EN_STEM = SnowballStemmer("english")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HeuristickÃ¡ detekce search-intent
def detect_intent(query: str) -> str:
    q = query.lower()
    trans_kw = {"koupit", "cena", "objednat", "recenze", "srovnÃ¡nÃ­", "discount"}
    if any(w in q for w in trans_kw):
        return "transactional"
    if q.startswith(("jak", "co", "proÄ", "kdy", "kde", "who", "how", "what", "why")):
        return "informational"
    return "informational"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# StahovÃ¡nÃ­ a analÃ½za HTML
def fetch_html(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (SEOOutlineBot/1.0; +https://example.com/bot)"
    }
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.ok:
            # rozpoznÃ¡nÃ­ kÃ³dovÃ¡nÃ­, prevence mojibake
            if not r.encoding or r.encoding.lower() == "utf-8":
                r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except requests.RequestException:
        pass
    return ""

def keyword_frequency(text: str, lang="cz", top_n: int = 20):
    # pouze pÃ­smena, min. 3 znaky (UNICODE)
    tokens = re.findall(r"\b[^\W\d_]{3,}\b", text.lower(), flags=re.UNICODE)
    tokens = [t for t in tokens if t not in STOP_WORDS]
    stemmer = CS_STEM if lang == "cz" else EN_STEM
    stems = [stemmer.stem(t) for t in tokens]

    counts = defaultdict(int)
    for stem in stems:
        counts[stem] += 1

    # mapujeme zpÄ›t na â€žnejreprezentativnÄ›jÅ¡Ã­â€œ pÅ¯vodnÃ­ slovo
    stem_to_word = {}
    for word in tokens:
        st = stemmer.stem(word)
        if st not in stem_to_word:
            stem_to_word[st] = word

    return [(stem_to_word[s], c) for s, c in Counter(counts).most_common(top_n)]

def get_lsi_keywords(docs, top_n: int = 12):
    vec = TfidfVectorizer(
        max_features=2000,
        ngram_range=(1, 2),
        stop_words=list(STOP_WORDS)
    )
    X = vec.fit_transform(docs)
    scores = X.sum(axis=0).A1
    idx = scores.argsort()[::-1][:top_n]
    feats = vec.get_feature_names_out()
    return [feats[i] for i in idx]

def analyse_page(url: str):
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.extract()
    text = " ".join(soup.stripped_strings)
    kw = keyword_frequency(text)
    return text[:2000], kw, text  # preview, KW, celÃ½ text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SerpAPI vyhledÃ¡vÃ¡nÃ­
def search_google(query: str, n: int = 3):
    search = GoogleSearch({
        "engine": "google",
        "q": query,
        "num": n,
        "api_key": SERP_API_KEY,
        "hl": "cs",
    })
    return search.get_dict().get("organic_results", [])[:n]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GenerovÃ¡nÃ­ osnovy pÅ™es OpenAI
def propose_outline(query, intent, top_kw, lsi_kw, analyses):
    system = (
        "You are an expert Czech SEO strategist. "
        "Generate ONLY a detailed outline (H1, H2, optional H3) with bullet-point notes, "
        "a meta-title (<=60 char) and meta-description (<=155 char). "
        "Also suggest 3-5 internal links (anchor text + slug). "
        "Do NOT write full paragraphs."
    )

    user = (
        f"Search query: {query}\n"
        f"Search intent: {intent}\n"
        f"Primary keywords: {', '.join(top_kw[:10])}\n"
        f"LSI keywords: {', '.join(lsi_kw)}\n\n"
        f"Competitor snapshot:\n"
    )
    for i, a in enumerate(analyses, 1):
        user += f"{i}. {a['url']}\n   KW: {', '.join(a['keywords'][:10])}\n"

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        max_tokens=900,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit UI
st.set_page_config(page_title="SEO Outline Generator", page_icon="ðŸ”")
st.title("ðŸ” SEO Article Outline Generator")

query = st.text_input("Zadej vyhledÃ¡vacÃ­ dotaz")

if query:
    if not SERP_API_KEY or not os.getenv("OPENAI_API_KEY"):
        st.error("âŒ ChybÃ­ SERPAPI_API_KEY nebo OPENAI_API_KEY.")
        st.stop()

    intent = detect_intent(query)
    st.caption(f"DetekovanÃ½ intent: **{intent}**")
    st.info("â³ VyhledÃ¡vÃ¡m konkurenciâ€¦")

    serp_results = search_google(query)
    if not serp_results:
        st.error("SerpAPI nevrÃ¡tilo Å¾Ã¡dnÃ© vÃ½sledky.")
        st.stop()

    analyses, all_docs = [], []

    for res in serp_results:
        url = res.get("link")
        title = res.get("title", url)

        st.subheader(title)
        ext = tldextract.extract(url)
        domain = ".".join(p for p in [ext.domain, ext.suffix] if p) or url
        st.caption(domain)

        preview, kw, full = analyse_page(url)
        all_docs.append(full)
        st.markdown("**Top klÃ­ÄovÃ¡ slova:** " + ", ".join(f"`{w}`" for w, _ in kw))
        with st.expander("UkÃ¡zka textu"):
            st.write(preview)

        analyses.append({"url": url, "keywords": [w for w, _ in kw]})

    # agregace KW + LSI
    combined = Counter()
    for a in analyses:
        combined.update(a["keywords"])
    top_kw = [w for w, _ in combined.most_common(40)]
    lsi_kw = get_lsi_keywords(all_docs)

    st.info("ðŸ“ Generuji osnovu ÄlÃ¡nkuâ€¦")
    outline_md = propose_outline(query, intent, top_kw, lsi_kw, analyses)

    st.markdown("---")
    st.subheader("ðŸ“„ VÃ½stup")
    st.markdown(outline_md, unsafe_allow_html=True)
    st.success("âœ… Osnova vygenerovÃ¡na!")
