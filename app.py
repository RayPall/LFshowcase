"""Streamlit SEO Article Idea Generator
Author: ChatGPT (OpenAI)
Date: 2025â€‘06â€‘12

Instructions
============
1. Export environment variables (or create a .env file) with your keys:
   export OPENAI_API_KEY="skâ€‘..."
   export SERPAPI_API_KEY="..."
2. Install dependencies:
   pip install streamlit requests beautifulsoup4 tldextract openai python-dotenv
3. Run the app:
   streamlit run seo_generator_streamlit.py

The app takes a search query in Czech, fetches the topâ€‘3 Google results via SerpAPI,
parses them, extracts headings & keywords, and uses the OpenAI API to draft a
1500â€‘word Markdown article structured for SEO.
"""

import collections
import os
import re
from urllib.parse import urlencode

import openai
import requests
import streamlit as st
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# ---------------------------
# Configuration & Constants
# ---------------------------
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
SERP_API_KEY = os.getenv("SERPAPI_API_KEY")

CZECH_STOPWORDS = {
    "a","i","ale","Å¾e","se","na","s","ve","v","je","jsem","jsi","jsou","byla","byli","bylo","by",
    "uÅ¾","do","pro","k","o","u","z","ze","si","tak","to","tento","tato","toto","tyto","jak","pÅ™i","od",
    "kterÃ½","kterÃ¡","kterÃ©","kdyÅ¾","proto","co","bÃ½t","mÃ­t","musÃ­","mÅ¯Å¾e","vÃ­ce","mÃ©nÄ›","ne","ano"
}

# ---------------------------
# Helper functions
# ---------------------------

def google_search(query: str, num_results: int = 3):
    """Return list of dicts with 'title', 'link', 'snippet' using SerpAPI."""
    params = {
        "engine": "google",
        "q": query,
        "api_key": SERP_API_KEY,
        "num": num_results,
        "hl": "cs",
    }
    url = f"https://serpapi.com/search?{urlencode(params)}"
    res = requests.get(url, timeout=15)
    res.raise_for_status()
    data = res.json()
    return [
        {
            "title": item.get("title"),
            "link": item.get("link"),
            "snippet": item.get("snippet"),
        }
        for item in data.get("organic_results", [])[:num_results]
    ]


def fetch_html(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0 (compatible; SEOGenerator/1.0)"}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        r.raise_for_status()
        return r.text
    except Exception:
        return ""


def extract_headings(soup: BeautifulSoup):
    headings = []
    for level in range(1, 7):
        for tag in soup.find_all(f"h{level}"):
            text = tag.get_text(strip=True)
            if text:
                headings.append({"level": level, "text": text})
    return headings


def keyword_frequency(text: str, top_n: int = 20):
    words = re.findall(r"\b[\wÃ¡-Å¾Ã-Å½]{4,}\b", text.lower())
    tokens = [w for w in words if w not in CZECH_STOPWORDS]
    return collections.Counter(tokens).most_common(top_n)


def analyze_page(html: str):
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.string.strip() if soup.title else ""
    meta_desc_tag = soup.find("meta", attrs={"name": "description"})
    meta_desc = meta_desc_tag["content"].strip() if meta_desc_tag else ""
    headings = extract_headings(soup)
    body_text = soup.get_text(" ", strip=True)
    keywords = keyword_frequency(body_text)
    return {
        "title": title,
        "meta_description": meta_desc,
        "headings": headings,
        "keywords": keywords,
        "word_count": len(body_text.split()),
    }


def summarize_keywords(analyses):
    combined = collections.Counter()
    for a in analyses:
        combined.update(dict(a["keywords"]))
    return combined.most_common(30)


def propose_article(query: str, top_keywords, analyses, language="cs") -> str:
    prompt = f"""
Jsi zkuÅ¡enÃ½ SEO specialista a copywriter.

NapiÅ¡ originÃ¡lnÃ­ SEO ÄlÃ¡nek v jazyce {language}, kterÃ½ cÃ­lÃ­ na klÃ­ÄovÃ© slovo "{query}" a mÃ¡ potenciÃ¡l umÃ­stit se v topÂ 3 Google.

Podklady kÂ analÃ½ze konkurence:
- ÄŒastÃ¡ klÃ­ÄovÃ¡ slova: {top_keywords}
- Struktura nadpisÅ¯ konkurence: {[a['headings'] for a in analyses]}

PoÅ¾adavky na ÄlÃ¡nek:
- ccaÂ 1500Â slov
- Struktura H1â€“H3 (pÅ™Ã­padnÄ› H4) sÂ vhodnÃ½m rozloÅ¾enÃ­m klÃ­ÄovÃ½ch slov (Å¾Ã¡dnÃ½ keyword stuffing)
- AtraktivnÃ­ Ãºvod, hlubokÃ¡ expertÃ­za, zÃ¡vÄ›r sÂ CTA
- NÃ¡vrh *meta title* do 60Â znakÅ¯ a *meta description* do 155Â znakÅ¯
- VraÅ¥ vÃ½sledek ve formÃ¡tu Markdown.
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2048,
    )
    return response.choices[0].message.content.strip()

# ---------------------------
# Streamlit UI
# ---------------------------

def main():
    st.set_page_config(page_title="SEO GenerÃ¡tor NÃ¡padÅ¯", layout="wide")
    st.title("ğŸ“ SEO GenerÃ¡tor NÃ¡padÅ¯ na ÄŒlÃ¡nky")

    query = st.text_input("Zadejte vyhledÃ¡vacÃ­ dotaz / klÃ­ÄovÃ© slovo", placeholder="napÅ™. jak vybrat elektrokolo")

    if st.button("Generovat") and query.strip():
        with st.spinner("ğŸ” VyhledÃ¡vÃ¡m konkurenci na Googluâ€¦"):
            results = google_search(query)
            if not results:
                st.error("NepodaÅ™ilo se zÃ­skat vÃ½sledky. Zkontrolujte API klÃ­Ä SerpAPI.")
                st.stop()

        st.subheader("TopÂ 3 vÃ½sledky")
        for res in results:
            st.markdown(f"- [{res['title']}]({res['link']}) â€” {res['snippet']}")

        analyses = []
        with st.spinner("ğŸ“Š Analyzuji strÃ¡nky konkurenceâ€¦"):
            for res in results:
                html = fetch_html(res["link"])
                analyses.append(analyze_page(html))

        st.subheader("SEO analÃ½za konkurence")
        for idx, analysis in enumerate(analyses, 1):
            with st.expander(f"VÃ½sledekÂ {idx}: {results[idx-1]['title']}"):
                st.write("**Titulek:**", analysis["title"])
                st.write("**Meta popisek:**", analysis["meta_description"] or "â€”")
                st.write("**PoÄet slov:**", analysis["word_count"])
                st.write("**TopÂ klÃ­ÄovÃ¡ slova:**", analysis["keywords"])
                st.write("**Nadpisy:**")
                for h in analysis["headings"]:
                    indent = "Â " * 2 * (h["level"]-1)
                    st.markdown(f"{indent}- H{h['level']} {h['text']}")

        top_keywords = summarize_keywords(analyses)

        with st.spinner("âœï¸ Generuji nÃ¡vrh ÄlÃ¡nkuâ€¦"):
            article_md = propose_article(query, top_keywords, analyses)

        st.subheader("ğŸ“„ NÃ¡vrh ÄlÃ¡nku")
        st.markdown(article_md)


if __name__ == "__main__":
    main()
