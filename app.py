"""
seo_article_generator.py

Streamlit aplikace, kter√°:
1) Vyhled√° TOP 3 v√Ωsledky Googlu p≈ôes SerpAPI
2) St√°hne jejich HTML (v≈ædy dek√≥duje p≈ôes apparent_encoding) a vyt√°hne kl√≠ƒçov√° slova
3) Vygeneruje **pouze osnovu** ƒçl√°nku (H1/H2/H3 + bullet-pointy),
   meta‚Äêtitle, meta‚Äêdescription a n√°vrhy intern√≠ch odkaz≈Ø.

Pot≈ôebn√© promƒõnn√© prost≈ôed√≠ / Streamlit Secrets:
  ‚Ä¢ SERPAPI_API_KEY
  ‚Ä¢ OPENAI_API_KEY
"""

import os
import re
import requests
from collections import Counter

import streamlit as st
from bs4 import BeautifulSoup
from serpapi import GoogleSearch
from openai import OpenAI
import tldextract

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# API kl√≠ƒçe & klient
SERP_API_KEY = os.getenv("SERPAPI_API_KEY")
client       = OpenAI()  # naƒçte OPENAI_API_KEY z env

# z√°kladn√≠ stop-slova (CZ + EN)
STOP_WORDS = {
    # English
    "the","a","an","and","or","of","to","in","on","for","with","is","are","be",
    "as","at","by","this","that","from","it","its","will","was","were","has","have",
    "had","but","not","your","you",
    # Czech
    "a","i","k","o","u","s","v","z","na","≈æe","se","je","jsou","by","byl","byla",
    "bylo","aby","do","od","po","pro","pod","nad","kter√Ω","kter√°","kter√©","co","to",
    "ten","ta","t√≠m","tuto","tu","jako","kde","kdy","jak","tak","tak√©","bez"
}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_html(url: str) -> str:
    """
    St√°hne HTML a v≈ædy dek√≥duje bajty p≈ôes apparent_encoding (ne .text),
    aby se p≈ôede≈°lo mojibake.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; SEOArticleBot/1.0)"
    }
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.ok:
            encoding = r.apparent_encoding or "utf-8"
            return r.content.decode(encoding, errors="replace")
    except requests.RequestException:
        pass
    return ""

def keyword_frequency(text: str, top_n: int = 20):
    """
    Extrahuje tokeny pouze z p√≠smen (min. 3 znaky),
    odfiltruje stop-slova a vr√°t√≠ top_n nejƒçastƒõj≈°√≠ch.
    """
    # \b hranice slova, [^\W\d_] = jen p√≠smena Unicode, {3,} minim√°lnƒõ t≈ôi
    tokens   = re.findall(r"\b[^\W\d_]{3,}\b", text.lower(), flags=re.UNICODE)
    filtered = [t for t in tokens if t not in STOP_WORDS]
    return Counter(filtered).most_common(top_n)

def analyse_page(url: str):
    """
    St√°hne str√°nku, odstran√≠ skripty/styly, vr√°t√≠ n√°hled textu a KW.
    """
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script","style","noscript"]):
        tag.extract()
    text = " ".join(soup.stripped_strings)
    kw   = keyword_frequency(text)
    return text[:2000], kw  # n√°hled prvn√≠ch 2 000 znak≈Ø + seznam (slovo, ƒçetnost)

def search_google(query: str, num_results: int = 3):
    """
    Vr√°t√≠ organick√© v√Ωsledky SerpAPI pro zadan√Ω dotaz.
    """
    params  = {
        "engine": "google",
        "q": query,
        "num": num_results,
        "api_key": SERP_API_KEY,
        "hl": "cs",
    }
    results = GoogleSearch(params).get_dict()
    return results.get("organic_results", [])[:num_results]

def propose_outline(query: str, top_keywords, analyses):
    """
    Vygeneruje **pouze osnovu** ƒçl√°nku v Markdownu:
      ‚Ä¢ H1/H2/H3 + bullet-pointy
      ‚Ä¢ meta-title (‚â§60 char), meta-description (‚â§155 char)
      ‚Ä¢ n√°vrh 3‚Äì5 intern√≠ch odkaz≈Ø
    """
    system = (
        "You are an expert Czech SEO strategist. "
        "Generate ONLY a detailed outline (H1, H2, optional H3) with bullet-point notes, "
        "a meta-title (<=60 chars) and meta-description (<=155 chars), "
        "and suggest 3‚Äì5 internal links (anchor text + slug). "
        "Do NOT write full paragraphs."
    )
    user = (
        f"Search query: {query}\n"
        f"Aggregated competitor keywords: {', '.join(top_keywords)}\n\n"
        "Competitor snapshot:\n"
    )
    for i, a in enumerate(analyses, 1):
        user += f"{i}. {a['url']}\n   keywords: {', '.join(a['keywords'])[:120]}\n"

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role":"system", "content": system},
            {"role":"user",   "content": user}
        ],
        max_tokens=1024,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Streamlit UI
st.set_page_config(page_title="SEO Article Outline Generator", page_icon="üîç")
st.title("üîç SEO Article Outline Generator")

query = st.text_input("Zadej vyhled√°vac√≠ dotaz", value="")
if query:
    if not SERP_API_KEY or not os.getenv("OPENAI_API_KEY"):
        st.error("‚ùå Chyb√≠ `SERPAPI_API_KEY` nebo `OPENAI_API_KEY`.")
        st.stop()

    st.info("‚è≥ Vyhled√°v√°m a analyzuji konkurenci‚Ä¶")
    results = search_google(query)
    if not results:
        st.error("Google/SerpAPI nevr√°til ≈æ√°dn√© v√Ωsledky.")
        st.stop()

    analyses = []
    for res in results:
        url   = res.get("link")
        title = res.get("title") or url

        st.subheader(title)
        try:
            ext = tldextract.extract(url)
            domain = ".".join(p for p in [ext.domain, ext.suffix] if p)
        except Exception:
            domain = url
        st.caption(domain)

        preview, kw = analyse_page(url)
        st.markdown(
            "**Top kl√≠ƒçov√° slova konkurence:** "
            + ", ".join(f"`{w}`" for w, _ in kw)
        )
        with st.expander("Uk√°zka textu"):
            st.write(preview)

        analyses.append({"url": url, "keywords": [w for w, _ in kw]})

    combined = Counter()
    for a in analyses:
        combined.update(a["keywords"])
    top_kw = [w for w, _ in combined.most_common(30)]

    st.info("üìù Generuji osnovu ƒçl√°nku‚Ä¶")
    outline_md = propose_outline(query, top_kw, analyses)

    st.markdown("---")
    st.subheader("üìÑ N√°vrh (outline) SEO ƒçl√°nku")
    st.markdown(outline_md, unsafe_allow_html=True)
    st.success("‚úÖ Osnova vygenerov√°na!")
